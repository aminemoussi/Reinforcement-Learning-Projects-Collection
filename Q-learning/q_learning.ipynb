{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OGcjCS2gOTSR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import re\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-learning is model based (no prior knowlegde of the env)\n",
        "\n",
        "based on Markov decision process\n",
        "\n",
        "used in situation with finite actions, states and steps\n",
        "\n",
        "works by exploring every action at every state and evaluates by assigning it a Q-Value\n",
        "\n",
        "Q-table stores best action at any state (main brain)\n"
      ],
      "metadata": {
        "id": "BUhiatFbOcaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid_size = 4\n",
        "start = (0, 0)\n",
        "goal = (3, 3)\n",
        "obstacle = [(1, 1), (2, 3), (3, 1)]\n",
        "#\n",
        "actions = [\n",
        "    (-1, 0),\n",
        "    (1, 0),\n",
        "    (0, -1),\n",
        "    (0, 1),  # up  # down  # left  # right\n",
        "]\n"
      ],
      "metadata": {
        "id": "Uuj9H_qKOY-l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensures we're within grid boundaries, and didnt run into an opstacle"
      ],
      "metadata": {
        "id": "N6i9pKEtOp3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_state(state: Tuple[int, int]) -> bool:\n",
        "    return 0 <= state[0] < grid_size and 0 <= state[1] < grid_size and state not in obstacle"
      ],
      "metadata": {
        "id": "0mdoPQ_oOmgY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "next state func to move from a given state toward a direction and checks if new state is valid\n"
      ],
      "metadata": {
        "id": "je7Ql1TYOwBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_next_state(state: Tuple[int, int], action: Tuple[int, int]) -> Tuple[int, int]:\n",
        "    new_state = (state[0] + action[0], state[1] + action[1])\n",
        "    return new_state if is_valid_state(new_state) else state"
      ],
      "metadata": {
        "id": "4LKBpvMNOt6Q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Q-Learning params:\n",
        "*   explr_rate: exploration 30% vs exploitation param 70%\n",
        "*   lr: for learning rate, how much of teh new info is kept, lr = 0.3 -> incorporate 30% of new info and retain 70% of old info when\n",
        " updating q-value\n",
        "*   disct_fct is the discount factor, gamma = 0.99 -> future estimated rewards are valuated at %99 of their actual value\n",
        "immediate rewards are considered a little more\n",
        "\n"
      ],
      "metadata": {
        "id": "38hvk-yBO19h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Learning params\n",
        "explr_rate = 0.3\n",
        "\n",
        "lr = 0.1\n",
        "\n",
        "disct_fct = 0.99\n",
        "episodes = 100000\n"
      ],
      "metadata": {
        "id": "w7XAqyXKO2iJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Rewards system: some feedback signal that inform the agent whether the actions are good or bad, so reward arriving at goal, and penalize hittig obstacles\n",
        "\n",
        "- get reward formula: goal-> +100, obstacle or wall -> -10, for each step -> -1 (to find the best path)\n"
      ],
      "metadata": {
        "id": "rlYZbgjGPdsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reward(state: Tuple[int, int], next_state: Tuple[int, int]) -> int:\n",
        "    if next_state == goal:\n",
        "        return 100\n",
        "    #elif (next_state in obstacle) oindicesr (next_state == state):\n",
        "    elif next_state == state:\n",
        "        return -10\n",
        "    else:\n",
        "        return -1"
      ],
      "metadata": {
        "id": "NM5DlkxkPeBj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- choose action determines whetehr the next move will be an exploratory move, or a conservative one picked from the q-table, this is the epsilon-greedy strategy\n",
        "\n",
        "- it assured continous learning even when the agent finds a good policy returns a move (up, down...)\n"
      ],
      "metadata": {
        "id": "0RKl27A0PmzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_action(state: Tuple[int, int], q_table: np.ndarray) -> Tuple[int, int]:\n",
        "    x, y = state\n",
        "    if random.uniform(0, 1) < explr_rate:\n",
        "        return random.choice(actions)\n",
        "    else:\n",
        "        return actions[np.argmax(q_table[x, y])]"
      ],
      "metadata": {
        "id": "cAHmkRLyPnLS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- q value of each action at a given state is calculated using belman equation\n",
        "\n",
        "- disct_fct * np.max(q_table[next_state]) is to factor in the future reward so best action is the one that maximizes both the immediate & future reward\n"
      ],
      "metadata": {
        "id": "JDGSxzTtQJsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_qtable(\n",
        "    q_table: np.ndarray,\n",
        "    state: Tuple[int, int],\n",
        "    action: Tuple[int, int],\n",
        "    next_state: Tuple[int, int],\n",
        "    reward: int,\n",
        "):\n",
        "    action_index = actions.index(action)\n",
        "    q_table[state][action_index] += lr * (\n",
        "        reward + disct_fct * np.max(q_table[next_state]) - q_table[state][action_index]\n",
        "    )"
      ],
      "metadata": {
        "id": "52L7McDCQKJ0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training process\n",
        "- 1 create a zeroed table same size as the grid, and start at (0, 0)\n",
        "- 2 choose action based on current state and q_table and ger next_state\n",
        "- 3 calculate reward + Q value\n",
        "- 4 go to next state and repeat"
      ],
      "metadata": {
        "id": "mWUsS8fPQRQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_agent() -> np.ndarray:\n",
        "    q_table = np.zeros((grid_size, grid_size, len(actions)))\n",
        "#\n",
        "    for _ in range(episodes):\n",
        "        state = start\n",
        "        steps = 0\n",
        "        max_steps_per_episode = 100  # Prevent infinite loops\n",
        "\n",
        "        while state != goal and steps < max_steps_per_episode:\n",
        "            action = choose_action(state, q_table)\n",
        "            next_state = get_next_state(state, action)\n",
        "            reward = get_reward(state, next_state)\n",
        "            update_qtable(q_table, state, action, next_state, reward)\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "#\n",
        "    return q_table\n",
        "#\n",
        "q_table = train_agent()\n",
        "print(q_table)"
      ],
      "metadata": {
        "id": "lke0vwDpQRhv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1bcbdad-5e80-4913-c1af-10277ff7df5c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 79.29602988  90.19800998  79.29602988  90.19800998]\n",
            "  [ 81.19800998  81.19800998  88.29602988  92.119202  ]\n",
            "  [ 83.119202    94.0598      90.19800998  90.19800998]\n",
            "  [ 81.19800888  92.11916561  92.119202    81.19800972]]\n",
            "\n",
            " [[ 88.29602988  92.119202    81.19800998  81.19800998]\n",
            "  [  0.           0.           0.           0.        ]\n",
            "  [ 92.119202    96.02        85.0598      92.119202  ]\n",
            "  [ 90.19800994  83.119202    94.0598      83.119202  ]]\n",
            "\n",
            " [[ 90.19800998  90.19800998  83.119202    94.0598    ]\n",
            "  [ 85.0598      85.0598      92.119202    96.02      ]\n",
            "  [ 94.0598      98.          94.0598      87.02      ]\n",
            "  [  0.           0.           0.           0.        ]]\n",
            "\n",
            " [[ 92.119202    81.19800998  81.19800998  81.19800998]\n",
            "  [  0.           0.           0.           0.        ]\n",
            "  [ 96.02        89.          89.         100.        ]\n",
            "  [  0.           0.           0.           0.        ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_q_table_as_grid(q_table: np.ndarray) -> None:\n",
        "    action_symbols = [\"^\", \">\", \"v\", \"<\"]\n",
        "    print(\"\\nQ-table Grid:\")\n",
        "    #\n",
        "    header = (\n",
        "        \"   |\"\n",
        "        + \"|\".join(\n",
        "            f\"   ({i},{j})   \" for i in range(grid_size) for j in range(grid_size)\n",
        "        )\n",
        "        + \"|\"\n",
        "    )\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "    #\n",
        "    for action_idx, action_symbol in enumerate(action_symbols):\n",
        "        row = f\" {action_symbol} |\"\n",
        "        for i in range(grid_size):\n",
        "            for j in range(grid_size):\n",
        "                if (i, j) == goal:\n",
        "                    cell = \"   GOAL    \"\n",
        "                elif (i, j) in obstacle:\n",
        "                    cell = \" OBSTACLE  \"\n",
        "                else:\n",
        "                    q_value = q_table[i, j, action_idx]\n",
        "                    cell = f\" {q_value:9.2f} \"\n",
        "                row += cell + \"|\"\n",
        "        print(row)\n",
        "        print(\"-\" * len(header))\n",
        "#\n",
        "#\n",
        "visualize_q_table_as_grid(q_table)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YHqlTWyQegY",
        "outputId": "e064b9a0-5c9a-468d-bce1-2b2b09b9f470"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q-table Grid:\n",
            "   |   (0,0)   |   (0,1)   |   (0,2)   |   (0,3)   |   (1,0)   |   (1,1)   |   (1,2)   |   (1,3)   |   (2,0)   |   (2,1)   |   (2,2)   |   (2,3)   |   (3,0)   |   (3,1)   |   (3,2)   |   (3,3)   |\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " ^ |     79.30 |     81.20 |     83.12 |     81.19 |     88.30 | OBSTACLE  |     92.12 |     90.20 |     90.20 |     85.06 |     94.06 | OBSTACLE  |     92.12 | OBSTACLE  |     96.02 |   GOAL    |\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " > |     90.20 |     81.20 |     94.06 |     92.12 |     92.12 | OBSTACLE  |     96.02 |     83.12 |     90.20 |     85.06 |     98.00 | OBSTACLE  |     81.20 | OBSTACLE  |     89.00 |   GOAL    |\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " v |     79.30 |     88.30 |     90.20 |     92.12 |     81.20 | OBSTACLE  |     85.06 |     94.06 |     83.12 |     92.12 |     94.06 | OBSTACLE  |     81.20 | OBSTACLE  |     89.00 |   GOAL    |\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " < |     90.20 |     92.12 |     90.20 |     81.20 |     81.20 | OBSTACLE  |     92.12 |     83.12 |     94.06 |     96.02 |     87.02 | OBSTACLE  |     81.20 | OBSTACLE  |    100.00 |   GOAL    |\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_policy(q_table: np.ndarray) -> None:\n",
        "    \"\"\"Visualize the learned policy as arrows\"\"\"\n",
        "    action_symbols = [\"↑\", \"↓\", \"←\", \"→\"]\n",
        "    print(\"\\n=== Learned Policy ===\")\n",
        "\n",
        "    for i in range(grid_size):\n",
        "        row = \"\"\n",
        "        for j in range(grid_size):\n",
        "            if (i, j) == goal:\n",
        "                row += \" G \"\n",
        "            elif (i, j) in obstacle:\n",
        "                row += \" X \"\n",
        "            else:\n",
        "                best_action = np.argmax(q_table[i, j])\n",
        "                row += f\" {action_symbols[best_action]} \"\n",
        "        print(row)\n",
        "\n",
        "visualize_policy(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGnb94FUbcn4",
        "outputId": "e8602b9e-56a0-4349-cb80-730ca972bc8e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Learned Policy ===\n",
            " ↓  →  ↓  ↓ \n",
            " ↓  X  ↓  ← \n",
            " →  →  ↓  X \n",
            " ↑  X  →  G \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_agent(q_table: np.ndarray) -> None:\n",
        "    \"\"\"Test the trained agent\"\"\"\n",
        "    print(\"\\n=== Testing Trained Agent ===\")\n",
        "    state = start\n",
        "    path = [state]\n",
        "    steps = 0\n",
        "    max_steps = 20\n",
        "\n",
        "    while state != goal and steps < max_steps:\n",
        "        action = choose_action(state, q_table)\n",
        "        next_state = get_next_state(state, action)\n",
        "        path.append(next_state)\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "\n",
        "    print(f\"Path from {start} to {goal}:\")\n",
        "    for i, pos in enumerate(path):\n",
        "        print(f\"Step {i}: {pos}\")\n",
        "    print(f\"Total steps: {len(path) - 1}\")\n",
        "\n",
        "    if state == goal:\n",
        "        print(\"✓ Successfully reached goal!\")\n",
        "    else:\n",
        "        print(\"✗ Failed to reach goal\")\n",
        "\n",
        "\n",
        "test_agent(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcmXVf1UbSn6",
        "outputId": "f22549f1-7ddd-4aae-9950-3a765a3738b4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Testing Trained Agent ===\n",
            "Path from (0, 0) to (3, 3):\n",
            "Step 0: (0, 0)\n",
            "Step 1: (1, 0)\n",
            "Step 2: (2, 0)\n",
            "Step 3: (2, 1)\n",
            "Step 4: (2, 2)\n",
            "Step 5: (3, 2)\n",
            "Step 6: (3, 3)\n",
            "Total steps: 6\n",
            "✓ Successfully reached goal!\n"
          ]
        }
      ]
    }
  ]
}