# Reinforcement Learning Projects Collection (will be uploaded soon, almost finished)

A collection of reinforcement learning implementations showcasing fundamental to advanced algorithms.

ğŸ“‹ Repository Structure
```
RL-Projects/
â”œâ”€â”€ 1-Q-Learning-GridWorld/
â”‚   â”œâ”€â”€ ql_earning.ipynb
â”‚   â”œâ”€â”€ media/
â”‚       â”œâ”€â”€ Q_Table_Visualization.png
â”‚       â”œâ”€â”€ agent_test.png
â”‚       â””â”€â”€ policy.png
â”œâ”€â”€ 2-DeepQ-Network-CartPole/
â”‚   â”œâ”€â”€ dqn_cartpole.ipynb
â”‚   â”œâ”€â”€ cartpole_model/
â”‚   â”‚   â””â”€â”€ DQN.keras
â”‚   â”œâ”€â”€ media/
â”‚   â”‚   â”œâ”€â”€ cart_pole.gif
â”‚   â”‚   â”œâ”€â”€ DQN_policy_viz.png
â”‚   â”‚   â””â”€â”€ dqn_net.png
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ 3-PPO-Pong/
â”‚   â”œâ”€â”€ ppo_pong.ipynb
â”‚   â”œâ”€â”€ ppo_models/
â”‚   â”‚   â”œâ”€â”€ pong_ppo_early.zip
â”‚   â”‚   â””â”€â”€ pong_ppo_best.zip
â”‚   â”œâ”€â”€ media/
â”‚   â”‚   â”œâ”€â”€ ponggif.gif
â”‚   â”‚   â”œâ”€â”€ ppo_model_arch.png
â”‚   â”‚   â”œâ”€â”€ action_prob_ppo.png
â”‚   â”‚   â””â”€â”€ preprocessing.png
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md (main repository README)
```

# ğŸ“ Projects Overview
## 1. ğŸ¤– Q-Learning: Grid World Navigation

This implementation demonstrates classic Q-Learning in a 4x4 grid environment with obstacles. The agent starts at (0,0) and learns to navigate to the goal at (3,3) while avoiding obstacles.

Key Features:
- Tabular Q-learning implementation
- Dynamic exploration-exploitation balance (Îµ-greedy)
- Policy visualization
- Custom grid environment with obstacles

Results:
 - Learned Policy: ![](Q-learning/media/policy.png)
 - Test Results: ![](Q-learning/media/agent_test.png)
 - Q-Table Visualization: ![](Q-learning/media/Q_Table_Visualization.png)

---------

## 2. ğŸ§  Double Deep Q-Network: CartPole Balancing

A PyTorch implementation of Deep Q-Network (DQN) with experience replay and target network stabilization to solve the CartPole-v1 environment from Gymnasium.

### ğŸ¯ Features
 - Double DQN - Reduces overestimation bias
 - Experience Replay - Learns from past experiences
 - Target Network - Stabilizes training
 - Epsilon-Greedy - Balanced exploration vs exploitation

### ğŸ“ˆ Training Results

https://cartpole_model/training_results.png

The agent typically solves CartPole-v1 (195+ average score) in 200-300 episodes using this configuration.


### ğŸ® Demo
https://demo/demo.gif

### ğŸš€ Quick Start
- Installation
```
pip install gymnasium torch matplotlib
```

- Training
```
python train.py
```

- Testing
```
python test.py
```

### ğŸ—ï¸ Architecture
 - Network: 3-layer MLP (24 â†’ 24 neurons)
 - Algorithm: Double DQN with experience replay
 - Optimizer: Adam (lr=0.001)
 - State Space: 4 dimensions
 - Action Space: 2 actions (left/right)

### ğŸ“ Project Structure
```
â”œâ”€â”€ agent.py          # DQN agent implementation
â”œâ”€â”€ ddqn.py           # Neural network architecture  
â”œâ”€â”€ train.py          # Training script
â”œâ”€â”€ test.py           # Testing/evaluation script
â””â”€â”€ cartpole_model/   # Saved models & training plots
```

--------


## 3. ğŸ® Proximal Policy Optimization: Atari Pong
Screenshot: media/action_prob_ppo.png

Key Features:
- Policy gradient optimization with clipping
- Convolutional neural network for pixel input
- Frame stacking and preprocessing
- Advantage estimation using GAE

Model Architecture:
- Input: 84x84x4 stacked frames
- CNN Backbone: 3 convolutional layers
- Heads: Actor (policy) + Critic (value)
- Training: 0M timesteps with stable learning

## ğŸ› ï¸ Installation
```
git clone https://github.com/yourusername/RL-Projects.git
cd RL-Projects
```

## Install dependencies
```
pip install -r requirements.txt
```

requirements.txt:
```
gymnasium==0.29.1
tensorflow==2.15.0
stable-baselines3==2.0.0
numpy==1.24.3
ale-py==0.9.0
opencv-python==4.8.1
wandb==0.16.1
```


## ğŸ“Š Algorithm Comparison
Algorithm	Type	State Space	Action Space	Best For
Q-Learning	Value-based	Discrete	Discrete	Small, tabular environments
DQN	Value-based	Continuous	Discrete	High-dimensional observations
PPO	Policy-based	Continuous	Discrete/Continuous	Complex, stochastic environments
