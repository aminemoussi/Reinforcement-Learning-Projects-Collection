# Reinforcement Learning Projects Collection (will be uploaded soon, almost finished)

A collection of reinforcement learning implementations showcasing fundamental to advanced algorithms. Perfect for understanding RL concepts and their practical applications in autonomous systems and decision-making.

üìã Repository Structure

```
RL-Projects/
‚îú‚îÄ‚îÄ 1-Q-Learning-GridWorld/
‚îÇ   ‚îú‚îÄ‚îÄ qlearning_gridworld.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ media/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ qgame_1.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ qgame_2.png
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ qtable.png
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ 2-DeepQ-Network-CartPole/
‚îÇ   ‚îú‚îÄ‚îÄ dqn_cartpole.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ cartpole_model/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DQN.keras
‚îÇ   ‚îú‚îÄ‚îÄ media/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cart_pole.gif
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DQN_policy_viz.png
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dqn_net.png
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ 3-PPO-Pong/
‚îÇ   ‚îú‚îÄ‚îÄ ppo_pong.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ ppo_models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pong_ppo_early.zip
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pong_ppo_best.zip
‚îÇ   ‚îú‚îÄ‚îÄ media/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ponggif.gif
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ppo_model_arch.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ action_prob_ppo.png
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessing.png
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md (main repository README)
```

# üìÅ Projects Overview
## 1. ü§ñ Q-Learning: Grid World Navigation
Screenshot: media/qgame_2.png

Key Features:
- Tabular Q-learning implementation
- Dynamic exploration-exploitation balance (Œµ-greedy)
- Optimal policy visualization
- Custom grid environment with obstacles

Results:
Best Actions Grid:
-------------------------------------------
| >:  94.06   | >:  96.02   | v:  98.00   | 
-------------------------------------------
| v:  96.02   |  OBSTACLE   | v: 100.00   | 
-------------------------------------------
| >:  98.00   | >: 100.00   |    GOAL     | 
-------------------------------------------

## 2. üß† Deep Q-Network: CartPole Balancing
Screenshot: media/DQN_policy_viz.png

Key Features:
- Neural network Q-value approximation
- Experience replay buffer
- Target network stabilization
- Double DQN implementation

Architecture:
Input (4) ‚Üí Dense(24) ‚Üí Dense(24) ‚Üí Output(2)
Total Parameters: 770

Performance: Achieved perfect score of 500/500 consistently

## 3. üéÆ Proximal Policy Optimization: Atari Pong
Screenshot: media/action_prob_ppo.png

Key Features:
- Policy gradient optimization with clipping
- Convolutional neural network for pixel input
- Frame stacking and preprocessing
- Advantage estimation using GAE

Model Architecture:
- Input: 84x84x4 stacked frames
- CNN Backbone: 3 convolutional layers
- Heads: Actor (policy) + Critic (value)
- Training: 0M timesteps with stable learning

## üõ†Ô∏è Installation
```
git clone https://github.com/yourusername/RL-Projects.git
cd RL-Projects
```

## Install dependencies
```
pip install -r requirements.txt
```

requirements.txt:
```
gymnasium==0.29.1
tensorflow==2.15.0
stable-baselines3==2.0.0
numpy==1.24.3
ale-py==0.9.0
opencv-python==4.8.1
wandb==0.16.1
```


## üìä Algorithm Comparison
Algorithm	Type	State Space	Action Space	Best For
Q-Learning	Value-based	Discrete	Discrete	Small, tabular environments
DQN	Value-based	Continuous	Discrete	High-dimensional observations
PPO	Policy-based	Continuous	Discrete/Continuous	Complex, stochastic environments
